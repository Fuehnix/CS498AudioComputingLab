{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fc809921ed1d6e2d5a49ca287ffd856",
     "grade": false,
     "grade_id": "cell-3bb84ceabd1c424a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 9: Keyword Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32a09da2e153bb9a2de7a5e976c8fabe",
     "grade": false,
     "grade_id": "cell-e3872b22807e8bf8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Making a Digit Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:15:16.212058Z",
     "start_time": "2020-04-08T15:15:16.184391Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c27303d6081793ae6b6a2146a459863",
     "grade": false,
     "grade_id": "cell-d8ff51ba4afba87c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section we will design a simple spoken digit recognizer, based on Dynamic Time Warping (DTW). In order to make such a system we need to first collect some data, and then design a DTW routine that can compare new inputs with templates for each digit.\n",
    "\n",
    "To start with make a set of data that will be used here. Make a dozen or so recordings of yourself speaking each of the ten digits (0 to 9). We will use one recording from each digit as the template, and the rest at testing data. In order to not spend too much time collecting the data, record all these utterances in a single (long) sound file. Use your voice activity detector to split that file into the individual spoken digits.\n",
    "\n",
    "In order to design a digit recognizer we will take a spoken input of a digit and compare it to each digit’s template. By finding which template is the most similar we can classify the input as belonging to that template’s digit. In order to measure the distance between the two sequences we have to use DTW on an appropriate feature space.\n",
    "\n",
    "Decide which feature to use to represent your speech signals. It can be any feature that we used in the past (e.g. some type of an STFT, MFCCs, etc). When comparing a template with a new input you need to perform the following steps:\n",
    "\n",
    "1. Compute the distance matrix between all the features of each input. This will be a $M$ by $N$ matrix in which the $(i, j)$ element will represent the distance between the $i$-th frame of the template and the $j$-th frame of the input. We will use the cosine distance which is defined as:\n",
    "\n",
    "$$D(\\mathbf{a},\\mathbf{b}) = \\frac{\\sum a_i b_i}{\\sqrt{a_i^2}\\sqrt{\\sum b_i^2}}$$\n",
    "\n",
    "2. Once you obtain the distance matrix, you need to compute the cost matrix that encodes the cost of passing through a node given a previously optimal path. We will use the local constraint that to reach node $(i, j)$ you can either come from nodes $(i–1, j–1)$, $(i, j–1)$ or $(i–1, j)$.\n",
    "\n",
    "3. Starting from the first element of the matrix (1,1), and for each element of the cost matrix you will need to perform the following steps. For node $(i, j)$ you need to examine the nodes from which you can reach it – these will be nodes $(i–1, j–1)$, $(i, j–1)$ or $(i–1, j)$ – and see which one has the lowest cost. Therefore, reaching that node from the optimal path will have the cost of the optimal preceding node plus the distance that corresponds to being at node $(i, j)$. Iterate until you calculate the cost of passing through every node. As you do that, for each node keep track of which of the three preceding nodes was the optimal one.\n",
    "\n",
    "4. Now you can backtrack and find the optimal path. Start from the final point of the cost matrix and find the node from which you arrived there (it will be the same one that had the lowest cost above). Once you get to that node, repeat this process until you reach the beginning indexes of the two sequences. The path that you took in this process will be the optimal path that aligns the two sequences.\n",
    "\n",
    "5. The distance between the two sequences will be the cost of being at the final node. Use this to perform the digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:19:24.792151Z",
     "start_time": "2020-04-08T15:19:24.784998Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687c4784245782bf1fd2c6d60565e11f",
     "grade": true,
     "grade_id": "cell-e33e6cfc6a3af362",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "import math\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wavfile\n",
    "import scipy.signal as sg\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def sound( x, rate=8000, label=''):\n",
    "    from IPython.display import display, Audio, HTML\n",
    "    display( HTML( \n",
    "    '<style> table, th, td {border: 0px; }</style> <table><tr><td>' + label + \n",
    "    '</td><td>' + Audio( x, rate=rate)._repr_html_()[3:] + '</td></tr></table>'\n",
    "    ))\n",
    "    \n",
    "def loadDir(directory):\n",
    "    audioData = []\n",
    "    rate = 0\n",
    "    for filename in tqdm(listdir(directory)):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            curFilePath = os.path.join(directory, filename)\n",
    "            #rate should be same for all files, only 1 variable needed\n",
    "            wavRate, wavData = wavfile.read(curFilePath)\n",
    "            audioData.append(wavData)\n",
    "            if rate != wavRate:\n",
    "                rate = wavRate\n",
    "    return audioData, rate\n",
    "\n",
    "def stft( input_sound, dft_size, hop_size, zero_pad, window):\n",
    "    # YOUR CODE HERE\n",
    "    padZeroes = np.zeros(dft_size)\n",
    "    pad = np.append(padZeroes, input_sound)\n",
    "    pad = np.append(pad, padZeroes)\n",
    "    tempMatrix = []\n",
    "    for i in range(0, len(pad)-dft_size, hop_size):\n",
    "        tempMatrix.append(pad[i:i+dft_size] * window)\n",
    "    out = []\n",
    "    for i in range(len(tempMatrix)):\n",
    "        rfft = np.fft.rfft(tempMatrix[i], dft_size+zero_pad)\n",
    "        out.append(np.reshape(rfft, (-1, 1)))     \n",
    "    return np.hstack(out)\n",
    "\n",
    "def istft( stft_output, dft_size, hop_size, zero_pad, window):\n",
    "    # YOUR CODE HERE\n",
    "    irfftSounds = []\n",
    "    for segment in stft_output.T:\n",
    "        irfftSounds.append(np.fft.irfft(segment, dft_size+zero_pad))\n",
    "    irfftSounds = np.array(irfftSounds)\n",
    "    input_sound = np.zeros(dft_size+hop_size*(len(irfftSounds)-1))\n",
    "    for i in range(len(irfftSounds)):\n",
    "        input_sound[i*hop_size:i*hop_size+dft_size] += (irfftSounds[i][:dft_size] * window)\n",
    "    input_sound = input_sound[dft_size:]\n",
    "    return input_sound\n",
    "\n",
    "def VAD(sound, stft, energy, num, activeFrames, alpha, threshold):\n",
    "    noise_frame_counter = 0\n",
    "    VADfilter = []\n",
    "    temp = np.zeros((num, stft.shape[0]), dtype=\"complex\")\n",
    "    for i in range(stft.shape[1]):\n",
    "        if activeFrames[i]:\n",
    "            mean = np.mean(np.abs(temp), axis=0) \n",
    "            VADfilter.append((np.abs(stft[:, i]) - alpha * mean).clip(min=0) * np.exp(1j * np.angle(stft[:, i])))\n",
    "        else:\n",
    "            temp[noise_frame_counter] = stft[:, i]\n",
    "            VADfilter.append(np.zeros(stft[:, i].shape[0]))\n",
    "            noise_frame_counter = (noise_frame_counter + 1) % num\n",
    "    return np.array(VADfilter).T\n",
    "\n",
    "def plotSpectrogramIndex(file, STFTresult, rate, label=''):\n",
    "    freq = np.linspace(0, rate/2, STFTresult.shape[0])\n",
    "    absSTFT = np.absolute(STFTresult)\n",
    "    plt.pcolormesh(np.arange(absSTFT.shape[1]),freq, np.log(absSTFT + np.e))\n",
    "    plt.title(label)\n",
    "    plt.xlabel('Ind')\n",
    "    plt.ylabel('Freq')\n",
    "    plt.show()\n",
    "    \n",
    "def plotSpectrogramTime(file, STFTresult, rate, label=''):\n",
    "    time = np.linspace(0, len(file)/rate, STFTresult.shape[1])\n",
    "    freq = np.linspace(0, rate/2, STFTresult.shape[0])\n",
    "    absSTFT = np.absolute(STFTresult)\n",
    "    plt.pcolormesh(time, freq, np.log(absSTFT + np.e))\n",
    "    plt.title(label)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Freq')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f215d8cd216ae28f30606a5184d282eb",
     "grade": false,
     "grade_id": "cell-426ee85bc554a52a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2. Making a voice-driven dialer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7544e45416ff9821d92b2960725531d1",
     "grade": false,
     "grade_id": "cell-6945f72b9e4edb8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Suppose you just started working for a phone company and the first thing they ask you is to make a hands-free interface for their phones so that people can dial in their friends by voice. During setup, the users speak the name of a contact and then associate it with a number to call. Make a system for which you use the full name of 4-5 of your friends, so that when you speak their name the system recognizes it (and thus could subsequently call their number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a10add51e91361731e05d42bf1525ef",
     "grade": true,
     "grade_id": "cell-47019af0f52dca63",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
